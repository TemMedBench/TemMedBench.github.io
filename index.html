<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Medical Reasoning Benchmarks, Multimodal Benchmark, Multimodal Learning, Vision and Language Dataset, Vision Language Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MRAG-Bench</title>
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="icon" href="static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  </head>
  <body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://temmedbench.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <!-- <div class="navbar-dropdown">
          <a class="navbar-item" href="https://gordonhu608.github.io/mqtllava/">
            MQT-LLaVA
          </a>
          <a class="navbar-item" href="https://gordonhu608.github.io/bliva/">
            BLIVA
          </a>
          <a class="navbar-item" href="https://gordonhu608.github.io/VALOR-Eval/">
            VALOR-Eval
          </a>
        </div> -->
      </div>
    </div>

  </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <!-- <span class="author-block">
                  <a href="https://gordonhu608.github.io/" target="_blank"><font color="#9fc5e8"><b>Wenbo Hu</b></font></a><sup>1</sup>&emsp;
                </span> -->
                <span class="author-block">
                  <!-- <a href="" target="_blank">Junyi Zhang</a><sup>1</sup>,&emsp; -->
                  <a href="">Junyi Zhang</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <!-- <a href="" target="_blank">Jia-Chen Gu</a><sup>1</sup>,&emsp; -->
                  <a href="">Jia-Chen Gu</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <!-- <a href="" target="_blank">Wenbo Hu</a><sup>1</sup>,&emsp; -->
                  <a href="">Wenbo Hu</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <!-- <a href="" target="_blank">Yu Zhou</a><sup>1</sup>,&emsp; -->
                   <a href="">Yu Zhou</a><sup>1</sup>,&emsp;
                </span>
                
                <br>
                <span class="author-block">
                  <!-- <a href="" target="_blank">Robinson Piramuthu</a><sup>2</sup>,&emsp; -->
                  <a href="">Robinson Piramuthu</a><sup>2</sup>,&emsp;
                </span>
                <span class="author-block">
                  <!-- <a href="" target="_blank">Nanyun Peng</a><sup>1</sup>&emsp; -->
                  <a href="">Nanyun Peng</a><sup>1</sup>&emsp;
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>UCLA&emsp;
                      <sup>2</sup>Amazon&emsp;
                    </span>
                    <!-- <span class="author-block"><br>ICLR 2025</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span> -->
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <!-- ICLR 2025 -->
                    </span>
                    
                  </div>
<!-- 
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>*</sup>Equal Leadership&emsp;
                      <sup>†</sup>Equal Contribution
                    </span>
                  </div> -->

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/uclanlp/TemMed-Bench" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>HF Dataset</span>
                    </a>
                  </span>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/Levi-ZJY/TemMed-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">TemMed-Bench</h2>

      <img src="static/images/Teaser_Figure.png" height="100%"/>
      
      <br>
      <br>

      <p><strong>TemMed-Bench</strong> features three primary highlights.</p>
      <ul style="list-style: disc; margin-left: 1.5em;">
        <li>
          <strong>Temporal reasoning focus</strong>: each sample in TemMed-Bench includes historical condition information,
          challenging models to analyze changes in patient conditions over time.
        </li>
        <li>
          <strong>Multi-image input</strong>: each sample in TemMed-Bench contains multiple images from different visits as input,
          emphasizing the need for models to process and reason over multiple images.
        </li>
        <li>
          <strong>Diverse task suite</strong>: TemMed-Bench comprises three tasks including VQA, report generation, and image-pair selection.
          These tasks are all built upon a test set consisting of 1,000 samples. Additionally, TemMed-Bench includes a knowledge corpus with over 17,000 instances.
        </li>
      </ul>
      
      <h2 class="hero-body has-text-centered">
        <!-- <br> -->
        <!-- Example scenarios from <span style="font-weight:bold;">MRAG-Bench</span>. Previous benchmarks mainly focused on retrieving from textual knowledge. 
        However, there are scenarios where retrieving correct textual knowledge is hard and not as useful as visual knowledge. -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">TemMed-Bench -- Composition</h2>
        <br>
        <img src="static/images/Task_Figure.png" height="100%"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Examples of the three tasks in TemMed-Bench. Each question in these tasks is designed to challenge LVLMs' ability to analyze condition changes, providing a comprehensive evaluation of their temporal medical image reasoning ability.
          </li>
        </ul>

        <br>
        <br>
        
        <img src="static/images/Benchmark_Comparison.png" height="100%"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Comparison with previous works. TemMed-Bench focuses on evaluating LVLMs in temporal reasoning over multiple medical images.
          </li>
        </ul>
        

        <!-- <br>
        <br>


        <img src="static/images/Data_Amount.png" style="width:45%;"/> 
        <img src="static/images/Keywords_Distribution.png" style="width:48.5%;"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Key statistics and keywords distribution of TemMed-Bench
          </li>
        </ul> -->

        
      </div>
    </div>
  </div>
</section>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">TemMed-Bench -- Composition</h2>
        <br>

        <img src="static/images/Data_Amount.png" style="width:45%;"/> 
        <img src="static/images/Keywords_Distribution.png" style="width:48.5%;"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Key statistics and keywords distribution of TemMed-Bench
          </li>
        </ul>

        
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing medical reasoning benchmarks for vision-language models primarily focus on analyzing a patient's condition based on an image from a single visit. However, this setting deviates significantly from real-world clinical practice, where doctors typically refer to a patient's historical conditions to provide a comprehensive assessment by tracking their changes over time. In this paper, we introduce <strong>TemMed-Bench</strong>, the first benchmark designed for analyzing changes in patients' conditions between different clinical visits, which evaluates the ability of large vision-language models (LVLMs) to reason over <strong>tem</strong>poral <strong>med</strong>ical images. <strong>TemMed-Bench</strong> consists of a test set comprising three tasks – visual question-answering (VQA), report generation, and image-pair selection - and a supplementary knowledge corpus of over 17,000 instances. With <strong>TemMed-Bench</strong>, we conduct an evaluation of twelve LVLMs, comprising six proprietary and six open-source models. Our results show that most LVLMs lack the ability to analyze patients’ condition changes over temporal medical images, and a large proportion perform only at a random-guessing level in the zero-shot setting. In contrast, GPT o3, o4-mini and Claude 3.5 Sonnet demonstrate comparatively decent performance, although they still fall short of the ideal level. To enhance the tracking of condition changes, we explore augmenting the input with both retrieved visual and textual modalities in the medical domain. Experimental results indicate that multi-modal retrieval augmentation yields notably higher performance gains than textual retrieval alone across most models on our benchmark, with the VQA task showing an average improvement of 2.59%. Overall, our work benchmarks LVLMs in real-world clinical practice, reveals their limitations in temporal medical image reasoning, and highlights the use of multi-modal retrieval augmentation as a potentially promising direction worth exploring to address this challenge.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper Results -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Evaluation Results</h2>

        <img src="static/images/Results_zeroshot.png" width="100%"/>
        
        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Evaluation results on TemMed-Bench in the zero-shot setting. TemMed-Bench clearly reveals the limitations of current LVLMs in temporal medical image reasoning. Most LVLMs perform at around the random guess level in the VQA and image-pair selection tasks, and achieve relatively low average scores in the report generation task.
          </li>
        </ul>
        
        <br>
        <br>

        <img src="static/images/Results_RAG.png" width="100%"/>
        
        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Evaluation results on TemMed-Bench with text-only and multi-modal retrieval augmentation (all using top-1 retrieval). 
            The evaluation results demonstrate that multi-modal retrieval augmentation generally yields greater performance improvements across most models compared to text-only retrieval augmentation.
            Notably, compared to their text-only counterparts, HealthGPT, Claude 3.5 Sonnet, and GPT-4o demonstrate substantial gains in the multi-modal setting, with increases in VQA accuracy of 10.85%, 7.90%, and 4.75%, and improvements in report generation average score of 0.73, 0.68, and 1.59, respectively.
          </li>
        </ul>

        <br>
        <br>
        
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hu2024mragbench,
          title={MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models},
          author={Hu, Wenbo and Gu, Jia-Chen and Dou, Zi-Yi and Fayyaz, Mohsen and Lu, Pan and Chang, Kai-Wei and Peng, Nanyun},
          journal={arXiv preprint arXiv:2410.08182},
          year={2024}
        }
      </code></pre>
    </div>
</section>
End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
