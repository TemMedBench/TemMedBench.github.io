<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Medical Reasoning Benchmarks, Multimodal Benchmark, Multimodal Learning, Vision and Language Dataset, Vision Language Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TemMed-Bench</title>
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="icon" href="static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  </head>
  <body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://temmedbench.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://mragbench.github.io/">
            MRAG-Bench
          </a>
        </div>
      </div>
    </div>

  </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://levi-zjy.github.io/" target="_blank">Junyi Zhang</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://jasonforjoy.github.io/" target="_blank">Jia-Chen Gu</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://gordonhu608.github.io/" target="_blank">Wenbo Hu</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                   <a href="https://yu-bryan-zhou.github.io/" target="_blank">Yu Zhou</a><sup>1</sup>,&emsp;
                </span>
                
                <br>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/rpiramuthu/" target="_blank">Robinson Piramuthu</a><sup>2</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://violetpeng.github.io/" target="_blank">Nanyun Peng</a><sup>1</sup>&emsp;
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>UCLA&emsp;
                      <sup>2</sup>Amazon&emsp;
                    </span>
                    <!-- <span class="author-block"><br>ICLR</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span> -->
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <!-- ICLR -->
                    </span>
                    
                  </div>
                  <!-- 
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>*</sup>Equal Leadership&emsp;
                      <sup>†</sup>Equal Contribution
                    </span>
                  </div> -->

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2509.25143" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/uclanlp/TemMed-Bench" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>HF Dataset</span>
                    </a>
                  </span>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/Levi-ZJY/TemMed-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">TemMed-Bench</h2>

      <img src="static/images/Teaser_Figure.png" height="100%"/>
      
      <br>
      <br>

      <p><strong>TemMed-Bench</strong> features three primary highlights.</p>
      <ul style="list-style: disc; margin-left: 1.5em;">
        <li>
          <strong>Temporal reasoning focus:</strong> Each sample in TemMed-Bench includes historical condition information, which challenges models to analyze changes in patient conditions over time.
        </li>
        <li>
          <strong>Multi-image input:</strong> Each sample in TemMed-Bench contains multiple images from different visits as input, emphasizing the need for models to process and reason over multiple images.
        </li>
        <li>
          <strong>Diverse task suite:</strong> TemMed-Bench comprises three tasks, including VQA, report generation, and image-pair selection. Additionally, TemMed-Bench includes a knowledge corpus with more than 17,000 instances to support retrieval-augmented generation (RAG).
        </li>
      </ul>
      
      <h2 class="hero-body has-text-centered">
        <!-- <br> -->
        
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">TemMed-Bench -- Composition</h2>
        <br>
        <img src="static/images/Task_Figure.png" height="100%"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Examples of the three tasks in TemMed-Bench. Each question in these tasks is designed to challenge LVLMs' ability to analyze condition changes, providing a comprehensive evaluation of their temporal medical image reasoning ability.
          </li>
        </ul>

        <br>
        <br>
        
        <img src="static/images/Benchmark_Comparison.png" height="100%"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Comparison with previous works. TemMed-Bench focuses on evaluating LVLMs in temporal reasoning over multiple medical images.
          </li>
        </ul>
        

        <!-- <br>
        <br>


        <img src="static/images/Data_Amount.png" style="width:45%;"/> 
        <img src="static/images/Keywords_Distribution.png" style="width:48.5%;"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Key statistics and keywords distribution of TemMed-Bench
          </li>
        </ul> -->

        
      </div>
    </div>
  </div>
</section>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">TemMed-Bench -- Composition</h2>
        <br>

        <img src="static/images/Data_Amount.png" style="width:45%;"/> 
        <img src="static/images/Keywords_Distribution.png" style="width:48.5%;"/> 

        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Key statistics and keywords distribution of TemMed-Bench
          </li>
        </ul>

        
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing medical reasoning benchmarks for vision-language models primarily focus on analyzing a patient's condition based on an image from a single visit. However, this setting deviates significantly from real-world clinical practice, where doctors typically refer to a patient's historical conditions to provide a comprehensive assessment by tracking their changes over time. In this paper, we introduce <strong>TemMed-Bench</strong>, the first benchmark designed for analyzing changes in patients' conditions between different clinical visits, which challenges large vision-language models (LVLMs) to reason over <strong>tem</strong>poral <strong>med</strong>ical images. <strong>TemMed-Bench</strong> consists of a test set comprising three tasks - visual question-answering (VQA), report generation, and image-pair selection - and a supplementary knowledge corpus of over 17,000 instances. With <strong>TemMed-Bench</strong>, we conduct an evaluation of twelve LVLMs, comprising six proprietary and six open-source models. Our results show that most LVLMs lack the ability to analyze patients' condition changes over temporal medical images, and a large proportion perform only at a random-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini and Claude 3.5 Sonnet demonstrate comparatively decent performance, though they have yet to reach the desired level. To enhance the tracking of condition changes, we explore augmenting the input with both retrieved visual and textual modalities in the medical domain. We also show that multi-modal retrieval augmentation yields notably higher performance gains than no retrieval and textual retrieval alone across most models on our benchmark, with the VQA task showing an average improvement of 2.59%. Overall, we compose a benchmark grounded on real-world clinical practice, and it reveals LVLMs' limitations in temporal medical image reasoning, as well as highlighting the use of multi-modal retrieval augmentation as a potentially promising direction worth exploring to address this challenge.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper Results -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Evaluation Results</h2>

        <img src="static/images/Results_zeroshot.png" width="100%"/>
        
        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Evaluation results on TemMed-Bench in the closed-book setting. TemMed-Bench clearly reveals the limitations of current LVLMs in temporal medical image reasoning. Most LVLMs perform at around the random guess level in the VQA and image-pair selection tasks, and achieve relatively low average scores in the report generation task.
          </li>
        </ul>
        
        <br>
        <br>

        <img src="static/images/Results_RAG.png" width="100%"/>
        
        <br>
        <br>

        <ul style="list-style: disc; margin-left: 1.5em; font-size: 18px;">
          <li style="text-align: left;">
            Evaluation results on TemMed-Bench with text-only and multi-modal retrieval augmentation (using top-1 retrieval). 
            The evaluation results demonstrate that multi-modal retrieval augmentation generally yields greater performance improvements across most models compared to text-only retrieval augmentation.
            Notably, compared to their text-only counterparts, HealthGPT, Claude 3.5 Sonnet, and GPT-4o demonstrate substantial gains in the multi-modal setting, with increases in VQA accuracy of 10.85%, 7.90%, and 4.75%, and improvements in report generation average score of 0.73, 0.68, and 1.59, respectively.
          </li>
        </ul>

        <br>
        <br>
        
      </div>
    </div>
  </div>
</section>




<!-- Paper Analysis -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Analysis and Discussion</h2> <br>
    </div>
    <div class="columns is-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-5">
        Ablation Study on Retrieval Methods
        <h2 class="content has-text-justified">
          Results indicate that pairwise image retrieval achieved the highest performance, primarily due to two factors. 
          First, for image-to-text retrieval, the report feature in TemMed-Bench does not correspond to a single image, as the report describes condition changes between two images. Consequently, directly calculating the feature similarity between the report and a single image introduces bias.
          Second, image-to-image retrieval relies solely on the similarity of current-visit images, ensuring similarity in current conditions between the target and retrieved instances, but does not guarantee similarity in condition changes.
          Therefore, in TemMed-Bench, only by considering both historical and current images during retrieval and ensuring similarity in both, can retrieved instances reflect similar condition changes.
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/Retrieval_Method.png" width="95%"/> </div>
        <br>

        <h2 class="title is-5">Impact of Top-k Retrieval Augmentation</h2>
        <h2 class="content has-text-justified">
           To further analyze the impact of varying top-k retrievals on augmentation performance, we conducted experiments to evaluate the performance of LVLMs under top-1 to top-5 retrieval augmentation settings.
           Notably, multi-modal retrieval augmentation consistently outperforms text-only retrieval augmentation across top-1 to top-5 settings, further confirming the effectiveness of incorporating multi-modal retrieved information in enhancing LVLM performance in the medical domain.
           <br>
           <br>
           Furthermore, by comparing the performance improvement from top-1 to top-5, we observe that GPT-4o demonstrates a significantly higher increase in accuracy (6.6%) compared to HealthGPT (2.37%). These results
           suggest that LVLMs with superior multi-image processing capabilities derive greater benefits from an increased number of retrieved instances, underscoring the importance of enhancing multi-image processing ability to fully leverage multi-modal retrieved information.
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/TopK.png" width="95%"/> </div>
        <br>

    
        
      </div>
    </div>
  </div>
</section>






<!--BibTex citation -->
  <section class="section hero" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{zhang2025temmedbenchevaluatingtemporalmedical,
            title={TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models}, 
            author={Junyi Zhang and Jia-Chen Gu and Wenbo Hu and Yu Zhou and Robinson Piramuthu and Nanyun Peng},
            year={2025},
            eprint={2509.25143},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2509.25143}, 
      }
      </code></pre>
    </div>
</section>
<!-- End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
